import { GoogleGenAI } from '@google/genai';
import type { ImageData, GeneratedVideo } from '../types';

// Gemini API service for video generation using Veo 3.1

// Load Gooreum-i reference image (small, cute white Pomeranian)
async function loadGooreumReferenceImage(): Promise<{ base64: string; mimeType: string } | null> {
    try {
        // Dynamically import the reference image (cute small white Pomeranian)
        const imageModule = await import('../assets/gooreum-reference.jpg');
        const imageUrl = imageModule.default;

        // Fetch the image and convert to base64
        const response = await fetch(imageUrl);
        const blob = await response.blob();

        return new Promise((resolve, reject) => {
            const reader = new FileReader();
            reader.onloadend = () => {
                const dataUrl = reader.result as string;
                const base64 = dataUrl.split(',')[1];
                resolve({ base64, mimeType: blob.type || 'image/jpeg' });
            };
            reader.onerror = reject;
            reader.readAsDataURL(blob);
        });
    } catch (error) {
        console.error('Failed to load Gooreum-i reference image:', error);
        return null;
    }
}

// Generate scene image using Nanobanana Pro (Gemini 2.5 Flash Image)
export async function generateSceneImage(
    apiKey: string,
    location: string,
    time: string,
    situation: string,
    onProgress: (message: string) => void,
    aspectRatio: '16:9' | '9:16' | '1:1' = '9:16',
    situationRefImage?: { base64: string; mimeType: string } | null
): Promise<ImageData> {
    const ai = new GoogleGenAI({ apiKey });

    // Map aspect ratio to description
    const aspectRatioDesc = aspectRatio === '16:9' ? 'horizontal landscape (16:9)'
        : aspectRatio === '9:16' ? 'vertical portrait (9:16)'
            : 'square (1:1)';

    // Check if "êµ¬ë¦„ì´" is mentioned - if so, load reference image
    const hasGooreumiKeyword = situation.includes('êµ¬ë¦„ì´');
    let gooreumRef: { base64: string; mimeType: string } | null = null;

    if (hasGooreumiKeyword) {
        onProgress('êµ¬ë¦„ì´ ì°¸ì¡° ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...');
        gooreumRef = await loadGooreumReferenceImage();
    }

    // Build the prompt
    let promptText: string;

    if (hasGooreumiKeyword && gooreumRef) {
        promptText = `Look at the reference image carefully. This is "Gooreum-i" (êµ¬ë¦„ì´), a TINY, CUTE, SMALL white Pomeranian dog. Create a high-quality, cinematic photograph with ${aspectRatioDesc} aspect ratio based on these details:

Location: ${location}
Time: ${time}
Situation: ${situation}

=== CRITICAL CHARACTER REQUIREMENTS FOR "Gooreum-i" (êµ¬ë¦„ì´) ===
1. SIZE: Gooreum-i is a VERY SMALL dog - about 20-25cm tall, easily fits in one hand, MUCH SMALLER than a human (about 1/8 to 1/10 of adult human height)
2. BREED: Small white Pomeranian (NOT Samoyed, NOT Spitz, NOT any large breed)
3. APPEARANCE (MUST MATCH REFERENCE IMAGE FOR BASIC FEATURES): 
   - Pure fluffy WHITE fur (like cotton or cloud)
   - Tiny compact body with round face
   - Small black button eyes and tiny black nose
   - Adorable puppy-like features
   - Very fluffy and round overall shape
   â˜…â˜…â˜… CLOTHING: Apply clothing as specified in the SITUATION description above â˜…â˜…â˜…
   - If the situation mentions specific clothing, dress Gooreum-i accordingly
   - The clothing should fit naturally on a small Pomeranian
4. SCALE REFERENCE: When with humans, Gooreum-i should be small enough to be held in arms or sit on a lap

=== REPORTER REQUIREMENT (MANDATORY) ===
â˜…â˜…â˜… The reporter MUST be a 20-year-old Korean female reporter â˜…â˜…â˜…
- Age: 20 years old (young, fresh-faced Korean woman)
- Appearance: Professional yet youthful Korean female reporter
- She MUST be holding a microphone with "Gureumi TV" text CLEARLY VISIBLE on it
- The microphone must have "Gureumi TV" written/printed on it
- The text "Gureumi TV" must be readable and clearly visible in the image
- The microphone should be a professional broadcast microphone
- Position the microphone so the "Gureumi TV" branding is facing the camera
- This is a branding requirement - DO NOT skip this!

=== OTHER REQUIREMENTS ===
- The dog must look EXACTLY like the reference image - a tiny, cute, fluffy white Pomeranian (apply clothing from situation description)

Style: Realistic, cinematic lighting, high resolution, detailed, Korean aesthetic, ${aspectRatio === '9:16' ? 'vertical' : aspectRatio === '16:9' ? 'horizontal' : 'square'} composition`;

        onProgress('êµ¬ë¦„ì´ ì°¸ì¡° ì´ë¯¸ì§€ë¥¼ ì ìš©í•˜ì—¬ ìƒì„± ì¤‘...');
    } else {
        promptText = `Create a high-quality, cinematic photograph scene with ${aspectRatioDesc} aspect ratio:
Location: ${location}
Time: ${time}
Situation: ${situation}

=== REPORTER REQUIREMENT (MANDATORY) ===
â˜…â˜…â˜… If there is a reporter in the scene, the reporter MUST be a 20-year-old Korean female â˜…â˜…â˜…
- Age: 20 years old (young, fresh-faced Korean woman)
- Appearance: Professional yet youthful Korean female reporter
- She MUST be holding a microphone with "Gureumi TV" text CLEARLY VISIBLE on it
- The microphone must have "Gureumi TV" written/printed on it
- The text "Gureumi TV" must be readable and clearly visible in the image
- The microphone should be a professional broadcast microphone
- Position the microphone so the "Gureumi TV" branding is facing the camera
- This is a branding requirement - DO NOT skip this!

Style: Realistic, cinematic lighting, high resolution, detailed, ${aspectRatio === '9:16' ? 'vertical' : aspectRatio === '16:9' ? 'horizontal' : 'square'} composition for short-form video`;
    }

    onProgress('Nanobanana Proë¡œ ì´ë¯¸ì§€ ìƒì„± ìš”ì²­ ì¤‘...');

    try {
        // Build contents array - include reference image if available
        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        let contents: any;

        if (hasGooreumiKeyword && gooreumRef) {
            // Include reference image with the prompt
            contents = [
                {
                    inlineData: {
                        mimeType: gooreumRef.mimeType,
                        data: gooreumRef.base64,
                    },
                },
                // Add situation reference image if provided
                ...(situationRefImage ? [{
                    inlineData: {
                        mimeType: situationRefImage.mimeType,
                        data: situationRefImage.base64,
                    },
                }] : []),
                {
                    text: situationRefImage
                        ? `${promptText}\n\n=== ADDITIONAL REFERENCE IMAGE ===\nAlso refer to the second image as a style/situation reference. Match the overall mood, lighting, and atmosphere from this reference while creating the scene.`
                        : promptText
                },
            ];
        } else if (situationRefImage) {
            // Only situation reference image (no Gooreum-i)
            contents = [
                {
                    inlineData: {
                        mimeType: situationRefImage.mimeType,
                        data: situationRefImage.base64,
                    },
                },
                { text: `${promptText}\n\n=== REFERENCE IMAGE ===\nRefer to the provided image as a style/situation reference. Match the overall mood, lighting, composition, and atmosphere from this reference while creating the scene.` },
            ];
            onProgress('ì°¸ê³  ì´ë¯¸ì§€ë¥¼ ì ìš©í•˜ì—¬ ìƒì„± ì¤‘...');
        } else {
            contents = promptText;
        }

        // Use Nanobanana Pro (gemini-2.5-flash-image) for image generation
        const response = await ai.models.generateContent({
            model: 'gemini-2.5-flash-image',
            contents: contents,
            config: {
                responseModalities: ['Text', 'Image'],
                // Set aspect ratio using imageConfig
                imageConfig: {
                    aspectRatio: aspectRatio,
                },
            },
        });

        onProgress('ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘...');

        // Find the image part in the response
        const parts = response.candidates?.[0]?.content?.parts;
        if (!parts) {
            throw new Error('ì‘ë‹µì—ì„œ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
        }

        let base64Data: string | null = null;
        let mimeType = 'image/png';

        for (const part of parts) {
            // eslint-disable-next-line @typescript-eslint/no-explicit-any
            const inlineData = (part as any).inlineData;
            if (inlineData) {
                base64Data = inlineData.data;
                mimeType = inlineData.mimeType || 'image/png';
                break;
            }
        }

        if (!base64Data) {
            throw new Error('ì´ë¯¸ì§€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì‘ë‹µì— ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.');
        }

        // Convert base64 to blob and create preview URL
        const binaryString = atob(base64Data);
        const bytes = new Uint8Array(binaryString.length);
        for (let i = 0; i < binaryString.length; i++) {
            bytes[i] = binaryString.charCodeAt(i);
        }
        const blob = new Blob([bytes], { type: mimeType });
        const preview = URL.createObjectURL(blob);

        // Create a File object from the blob
        const file = new File([blob], 'generated-image.png', { type: mimeType });

        // Get dimensions from the image
        const dimensions = await new Promise<{ width: number; height: number }>((resolve) => {
            const img = new Image();
            img.onload = () => {
                resolve({ width: img.width, height: img.height });
            };
            img.src = preview;
        });

        onProgress('ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ!');

        return {
            file,
            preview,
            width: dimensions.width,
            height: dimensions.height,
            base64: base64Data,
            mimeType: mimeType,
            aspectRatio: aspectRatio,
        };
    } catch (error) {
        console.error('Image generation error:', error);
        throw new Error(`ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: ${error instanceof Error ? error.message : String(error)}`);
    }
}

// Helper function to download video with multiple fallback methods
async function downloadVideoFromResponse(
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    videoResponse: any,
    apiKey: string,
    onProgress: (message: string) => void
): Promise<Blob> {
    // Log available properties for debugging
    console.log('Video response object:', videoResponse);
    console.log('Video response keys:', Object.keys(videoResponse || {}));

    // Method 1: Check for inline video bytes (base64 encoded)
    if (videoResponse?.videoBytes) {
        onProgress('ë¹„ë””ì˜¤ ë°ì´í„° ì²˜ë¦¬ ì¤‘...');
        try {
            const binaryString = atob(videoResponse.videoBytes);
            const bytes = new Uint8Array(binaryString.length);
            for (let i = 0; i < binaryString.length; i++) {
                bytes[i] = binaryString.charCodeAt(i);
            }
            return new Blob([bytes], { type: 'video/mp4' });
        } catch (e) {
            console.error('Failed to decode video bytes:', e);
        }
    }

    // Method 2: Check for video data in different property names
    const possibleDataProps = ['data', 'content', 'bytes', 'videoData', 'inlineData'];
    for (const prop of possibleDataProps) {
        if (videoResponse?.[prop]) {
            onProgress(`ë¹„ë””ì˜¤ ë°ì´í„° ì²˜ë¦¬ ì¤‘ (${prop})...`);
            try {
                const data = videoResponse[prop];
                if (typeof data === 'string') {
                    // Assume base64
                    const binaryString = atob(data);
                    const bytes = new Uint8Array(binaryString.length);
                    for (let i = 0; i < binaryString.length; i++) {
                        bytes[i] = binaryString.charCodeAt(i);
                    }
                    return new Blob([bytes], { type: 'video/mp4' });
                } else if (data instanceof ArrayBuffer) {
                    return new Blob([data], { type: 'video/mp4' });
                } else if (data instanceof Uint8Array) {
                    return new Blob([data.slice()], { type: 'video/mp4' });
                } else if (typeof data === 'object' && data.data) {
                    // Handle nested data object
                    const binaryString = atob(data.data);
                    const bytes = new Uint8Array(binaryString.length);
                    for (let i = 0; i < binaryString.length; i++) {
                        bytes[i] = binaryString.charCodeAt(i);
                    }
                    return new Blob([bytes], { type: 'video/mp4' });
                }
            } catch (e) {
                console.error(`Failed to process ${prop}:`, e);
            }
        }
    }

    // Method 3: Try fetching from URI if available
    const uri = videoResponse?.uri || videoResponse?.url || videoResponse?.downloadUri;
    if (uri) {
        onProgress('ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì‹œë„ ì¤‘...');
        console.log('Attempting to fetch from URI:', uri);

        // Add API key to the URI for authentication
        const separator = uri.includes('?') ? '&' : '?';
        const authenticatedUri = `${uri}${separator}key=${apiKey}`;
        console.log('Fetching with authenticated URI');

        try {
            const response = await fetch(authenticatedUri, {
                method: 'GET',
                headers: {
                    'Accept': 'video/mp4,video/*,*/*',
                },
            });

            if (response.ok) {
                const blob = await response.blob();
                if (blob.size > 0) {
                    console.log('Successfully downloaded video, size:', blob.size);
                    return blob;
                }
            }
            console.log(`Fetch failed with status:`, response.status, response.statusText);
        } catch (e) {
            console.error(`Fetch error:`, e);
        }
    }

    // If we get here, we couldn't get the video
    const availableKeys = Object.keys(videoResponse || {}).join(', ');
    throw new Error(`ë¹„ë””ì˜¤ë¥¼ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì†ì„±: ${availableKeys || 'none'}`);
}

// Build enhanced video prompt with critical rules (for sequential generation - legacy)
function buildEnhancedVideoPrompt(
    userPrompt: string,
    sceneNumber: number,
    totalScenes: number,
    isLastScene: boolean
): string {
    // Parse the prompt to detect dialogue tags
    const parsed = parseScenePrompt(userPrompt);
    const hasDialogue = parsed.type === 'reporter' || parsed.type === 'goreumi' || userPrompt.includes('"');
    const isSituationOnly = parsed.type === 'situation';
    const isFirstScene = sceneNumber === 1;

    // START with voice instruction - this is the most important thing
    let enhancedPrompt = '';

    // Voice instruction FIRST - before anything else
    if (isFirstScene) {
        enhancedPrompt = `â˜…â˜…â˜… êµ¬ë¦„ì´ ëª©ì†Œë¦¬: í•œêµ­ 4ì„¸ ì—¬ì ì•„ê¸° ëª©ì†Œë¦¬ â˜…â˜…â˜…
Voice for Gooreum-i: Use a VERY HIGH-PITCHED 4-year-old Korean baby girl voice.
- Pitch: EXTREMELY HIGH like a real toddler (NOT an adult pretending)
- Style: "ì¡°ì•„ì¡°ì•„~!", "í—¤í—¤~", "~í•´ìš”~", baby lisp pronunciation
- This voice MUST continue EXACTLY the same in scenes 2, 3, 4!

=== ğŸ”‡ ì˜¤ë””ì˜¤ ê·œì¹™ (ë§¤ìš° ì¤‘ìš”!) ===
1. ì´ ì˜ìƒì—ëŠ” ì˜¤ì§ êµ¬ë¦„ì´ì™€ ë¦¬í¬í„°ì˜ ëª©ì†Œë¦¬ë§Œ í¬í•¨ë©ë‹ˆë‹¤
2. ë°°ê²½ ìŒì•… ì ˆëŒ€ ê¸ˆì§€ (NO background music)
3. íš¨ê³¼ìŒ ì ˆëŒ€ ê¸ˆì§€ (NO sound effects)
4. í™˜ê²½ìŒ/ì£¼ë³€ ì†Œë¦¬ ì ˆëŒ€ ê¸ˆì§€ (NO ambient sounds)

=== ğŸš« ìë§‰/í…ìŠ¤íŠ¸ ê¸ˆì§€ ===
- ìë§‰ ì—†ìŒ (NO subtitles)
- í™”ë©´ì— ê¸€ì ì—†ìŒ (NO on-screen text)
- ì˜¤ì§ ë§ˆì´í¬ì˜ "Gureumi TV" í…ìŠ¤íŠ¸ë§Œ í—ˆìš©

=== ì¥ë©´ ì„¤ëª… íƒœê·¸ ê·œì¹™ ===
- "ë¦¬í¬í„° : " = ë¦¬í¬í„°ì˜ ëŒ€ì‚¬ (ë¦¬í¬í„° ëª©ì†Œë¦¬ë¡œ ë§í•˜ê¸°)
- "êµ¬ë¦„ì´ : " = êµ¬ë¦„ì´ì˜ ëŒ€ì‚¬ (êµ¬ë¦„ì´ ëª©ì†Œë¦¬ë¡œ ë§í•˜ê¸°)
- "ìƒí™© : " = ëŒ€ì‚¬ ì—†ì´ í–‰ë™ë§Œ í‘œí˜„ (ì™„ì „ ë¬´ìŒ, ì›€ì§ì„ë§Œ)

[SCENE ${sceneNumber}/${totalScenes}]
${isSituationOnly ? `â˜… ìƒí™© ì¥ë©´ (ì™„ì „ ë¬´ìŒ) â˜…\n${parsed.content}\n- ì´ ì¥ë©´ì—ì„œëŠ” ì–´ë–¤ ì†Œë¦¬ë„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”` : userPrompt}

=== VOICE RULES ===
1. GOOREUM-I VOICE (êµ¬ë¦„ì´ ëª©ì†Œë¦¬):
   - 4ì„¸ í•œêµ­ ì—¬ì ì•„ê¸°ì²˜ëŸ¼ ì—„ì²­ ë†’ì€ ìŒìœ¼ë¡œ ë§í•´ì•¼ í•¨
   - ë°œìŒ: í˜€ ì§§ì€ ì†Œë¦¬ (ì¢‹ì•„â†’ì¡°ì•„, ë­í•´â†’ë¨¸í•´)
   - ë§íˆ¬: "~í•´ìš”", "~ì˜ˆìš”?", "íˆíˆ", "ì—í—¤í—¤~"
   - ì ˆëŒ€ ì„±ìˆ™í•˜ê±°ë‚˜ ì–´ë¥¸ìŠ¤ëŸ½ê²Œ ë“¤ë¦¬ë©´ ì•ˆ ë¨
   
2. REPORTER VOICE (ë¦¬í¬í„° ëª©ì†Œë¦¬):
   - 20ì‚´ í•œêµ­ ì—¬ì„± ë¦¬í¬í„° (ì Šê³  ë°œë„í•œ ëª©ì†Œë¦¬)
   - 20-year-old Korean female reporter
   - Professional yet youthful broadcasting voice
   - ë§ˆì´í¬ì— "Gureumi TV" í‘œì‹œ í•„ìˆ˜!`;
    } else {
        // For scenes 2, 3, 4 - emphasize CONTINUATION
        enhancedPrompt = `âš ï¸âš ï¸âš ï¸ ì¤‘ìš”: êµ¬ë¦„ì´ ëª©ì†Œë¦¬ë¥¼ ì¥ë©´1ê³¼ ë˜‘ê°™ì´ ìœ ì§€í•˜ì„¸ìš”! âš ï¸âš ï¸âš ï¸
CRITICAL: Continue Gooreum-i's voice EXACTLY from Scene 1!
- SAME high pitch as Scene 1 (í•œêµ­ 4ì„¸ ì—¬ì ì•„ê¸°)
- SAME baby-like speaking style
- DO NOT make the voice more mature or deeper!
- If voice changes at all = REJECTED

=== ğŸ”‡ ì˜¤ë””ì˜¤ ê·œì¹™ ===
- ì˜¤ì§ êµ¬ë¦„ì´ì™€ ë¦¬í¬í„° ëª©ì†Œë¦¬ë§Œ (ë°°ê²½ìŒì•…/íš¨ê³¼ìŒ/ì£¼ë³€ì†Œë¦¬ ê¸ˆì§€)
- "ìƒí™© :" ì¥ë©´ì€ ì™„ì „ ë¬´ìŒ

=== ğŸš« ìë§‰/í…ìŠ¤íŠ¸ ê¸ˆì§€ ===
- í™”ë©´ì— ì–´ë–¤ ê¸€ìë„ ë„£ì§€ ë§ˆì„¸ìš” (ë§ˆì´í¬ì˜ "Gureumi TV"ë§Œ í—ˆìš©)

[SCENE ${sceneNumber}/${totalScenes}] - ì´ì–´ì„œ (CONTINUATION)
${isSituationOnly ? `â˜… ìƒí™© ì¥ë©´ (ì™„ì „ ë¬´ìŒ) â˜…\n${parsed.content}\n- ì´ ì¥ë©´ì—ì„œëŠ” ì–´ë–¤ ì†Œë¦¬ë„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”` : userPrompt}

=== ëª©ì†Œë¦¬ ì—°ì†ì„± ê·œì¹™ (VOICE CONTINUITY) ===
1. êµ¬ë¦„ì´ ëª©ì†Œë¦¬ = ì¥ë©´1ê³¼ 100% ë™ì¼í•´ì•¼ í•¨
   - ì¥ë©´1ì˜ ëª©ì†Œë¦¬ë¥¼ ê·¸ëŒ€ë¡œ ì´ì–´ê°€ì„¸ìš”
   - ìŒë†’ì´: ì¥ë©´1ê³¼ ë˜‘ê°™ì´ ë†’ê²Œ (4ì„¸ ì•„ê¸°)
   - ë§íˆ¬: ì¥ë©´1ê³¼ ë˜‘ê°™ì´ (ì¡°ì•„ì¡°ì•„, í—¤í—¤, ~í•´ìš”)
   - ì ˆëŒ€ ëª©ì†Œë¦¬ê°€ ë‚®ì•„ì§€ê±°ë‚˜ ì„±ìˆ™í•´ì§€ë©´ ì•ˆ ë¨!
   
   Voice must be IDENTICAL to Scene 1:
   - Same HIGH pitch (4-year-old baby girl)
   - Same cute baby speech pattern
   - NEVER deeper, NEVER more mature
   
2. ë¦¬í¬í„° ëª©ì†Œë¦¬ = ì¥ë©´1ê³¼ ë™ì¼ (20ì‚´ í•œêµ­ ì—¬ì„±)
   - 20-year-old Korean female reporter voice`;
    }

    // Add posture rule
    enhancedPrompt += `

=== ìì„¸ ë° ì™¸ëª¨ ê·œì¹™ (POSTURE & APPEARANCE) ===
- êµ¬ë¦„ì´: ì•‰ê±°ë‚˜ ëˆ„ìš´ ìƒíƒœ ìœ ì§€ (ì¼ì–´ì„œì§€ ì•Šê¸°)
- Gooreum-i must stay SEATED or LYING DOWN
- â˜… êµ¬ë¦„ì´ ì™¸ëª¨: ì‘ê³  ê·€ì—¬ìš´ í•˜ì–€ í¬ë©”ë¼ë‹ˆì•ˆ (ìƒí™©ì—ì„œ ì§€ì •í•œ ì˜· ì°©ìš©) â˜…
- Gooreum-i is a small cute white Pomeranian (apply clothing from scene description)`;

    // Add dialogue rules
    if (hasDialogue && !isSituationOnly) {
        enhancedPrompt += `

=== ëŒ€ì‚¬ ê·œì¹™ (DIALOGUE) ===
- ë§ˆì´í¬ë¥¼ ë§í•˜ëŠ” ìºë¦­í„° ì… ê·¼ì²˜ì— ìœ ì§€
- Microphone stays near speaking character's mouth`;
    }

    // Smiling rules
    if (!isLastScene) {
        enhancedPrompt += `

=== í‘œì • (EXPRESSION) ===
- ì´ ì¥ë©´ì—ì„œëŠ” ì›ƒì§€ ì•Šê¸° (ë§ˆì§€ë§‰ ì¥ë©´ì—ì„œë§Œ ì›ƒìŒ)
- No smiling in this scene`;
    } else {
        enhancedPrompt += `

=== ë§ˆì§€ë§‰ ì¥ë©´ (FINAL SCENE) ===
- ëŒ€ì‚¬ ëë‚œ í›„ì—ë§Œ í•¨ê»˜ ë¯¸ì†Œ
- Smile together only AFTER dialogue ends`;
    }

    // End with voice reminder
    enhancedPrompt += `

=== ìµœì¢… í™•ì¸ ===
âœ“ êµ¬ë¦„ì´ ëª©ì†Œë¦¬ = í•œêµ­ 4ì„¸ ì—¬ì ì•„ê¸° (ì¥ë©´1ê³¼ ë™ì¼)
âœ“ Gooreum-i voice = 4-year-old Korean baby girl (same as Scene 1)
âœ“ ë°°ê²½ìŒì•…/íš¨ê³¼ìŒ/ì£¼ë³€ì†Œë¦¬ ì—†ìŒ
âœ“ ìë§‰/í…ìŠ¤íŠ¸ ì—†ìŒ`;

    return enhancedPrompt;
}

// Parse scene prompt to detect dialogue tags (ë¦¬í¬í„° : , êµ¬ë¦„ì´ : , ìƒí™© : )
function parseScenePrompt(prompt: string): { type: 'reporter' | 'goreumi' | 'situation' | 'mixed'; content: string } {
    const trimmedPrompt = prompt.trim();

    // Check for "ìƒí™© : " tag first (situation/action only, no dialogue)
    if (trimmedPrompt.startsWith('ìƒí™© :') || trimmedPrompt.startsWith('ìƒí™©:')) {
        const content = trimmedPrompt.replace(/^ìƒí™©\s*:\s*/, '');
        return { type: 'situation', content };
    }

    // Check for "ë¦¬í¬í„° : " tag (reporter dialogue)
    if (trimmedPrompt.startsWith('ë¦¬í¬í„° :') || trimmedPrompt.startsWith('ë¦¬í¬í„°:')) {
        const content = trimmedPrompt.replace(/^ë¦¬í¬í„°\s*:\s*/, '');
        return { type: 'reporter', content };
    }

    // Check for "êµ¬ë¦„ì´ : " tag (Goreumi dialogue)
    if (trimmedPrompt.startsWith('êµ¬ë¦„ì´ :') || trimmedPrompt.startsWith('êµ¬ë¦„ì´:')) {
        const content = trimmedPrompt.replace(/^êµ¬ë¦„ì´\s*:\s*/, '');
        return { type: 'goreumi', content };
    }

    // Mixed or untagged content
    return { type: 'mixed', content: trimmedPrompt };
}

// Build unified video prompt for single video with multiple scenes
// This ensures voice consistency by generating everything in one video
function buildUnifiedVideoPrompt(
    prompts: string[],
    durationSeconds: number
): string {
    const sceneCount = prompts.length;
    const secondsPerScene = durationSeconds / sceneCount;

    // Build scene descriptions with tag processing
    const sceneDescriptions = prompts.map((prompt, index) => {
        const startTime = index * secondsPerScene;
        const endTime = (index + 1) * secondsPerScene;
        const isLastScene = index === sceneCount - 1;

        // Parse the prompt to detect dialogue tags
        const parsed = parseScenePrompt(prompt);

        let sceneText = `[ì¥ë©´ ${index + 1}] (${startTime}ì´ˆ ~ ${endTime}ì´ˆ)\n`;

        // Handle different scene types
        if (parsed.type === 'situation') {
            // Situation only - NO AUDIO, just movement/action
            sceneText += `â˜… ìƒí™© ì¥ë©´ (ì†Œë¦¬ ì—†ìŒ) â˜…
${parsed.content}
- ì´ ì¥ë©´ì—ì„œëŠ” ì–´ë–¤ ì†Œë¦¬ë„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš” (ëŒ€ì‚¬, íš¨ê³¼ìŒ, ë°°ê²½ìŒ ëª¨ë‘ ì—†ìŒ)
- ì˜¤ì§ ì›€ì§ì„ê³¼ í–‰ë™ë§Œ í‘œí˜„í•˜ì„¸ìš”`;
        } else if (parsed.type === 'reporter') {
            // Reporter dialogue
            sceneText += `â˜… ë¦¬í¬í„° ëŒ€ì‚¬ â˜…
ë¦¬í¬í„°: ${parsed.content}
- ë¦¬í¬í„°ê°€ ë§ˆì´í¬ë¥¼ ì… ê°€ê¹Œì´ ëŒ€ê³  ë§í•©ë‹ˆë‹¤`;
        } else if (parsed.type === 'goreumi') {
            // Goreumi dialogue
            sceneText += `â˜… êµ¬ë¦„ì´ ëŒ€ì‚¬ â˜…
êµ¬ë¦„ì´: ${parsed.content}
- ë§ˆì´í¬ë¥¼ êµ¬ë¦„ì´ ì… ê°€ê¹Œì´ ëŒ€ê³ , êµ¬ë¦„ì´ê°€ ë§í•©ë‹ˆë‹¤`;
        } else {
            // Mixed or untagged - use original prompt
            sceneText += parsed.content;
        }

        if (!isLastScene) {
            sceneText += `\n(ì´ ì¥ë©´ì—ì„œëŠ” ì›ƒì§€ ì•Šê¸°)`;
        } else {
            sceneText += `\n(ëŒ€ì‚¬ê°€ ëë‚œ í›„ í•¨ê»˜ ë°ê²Œ ë¯¸ì†Œì§“ê¸°)`;
        }

        return sceneText;
    }).join('\n\n');

    const unifiedPrompt = `â˜…â˜…â˜… ì¤‘ìš”: ${durationSeconds}ì´ˆ ê¸¸ì´ì˜ ì˜ìƒì„ ìƒì„±í•©ë‹ˆë‹¤ â˜…â˜…â˜…
ì´ ì˜ìƒì€ ${sceneCount}ê°œì˜ ì¥ë©´ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ê° ì¥ë©´ì€ ì•½ ${secondsPerScene}ì´ˆì…ë‹ˆë‹¤.

=== ğŸ”‡ ì˜¤ë””ì˜¤ ê·œì¹™ (ë§¤ìš° ì¤‘ìš”!) ===
â˜…â˜…â˜… ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ê·œì¹™ â˜…â˜…â˜…
1. ì´ ì˜ìƒì—ëŠ” ì˜¤ì§ êµ¬ë¦„ì´ì™€ ë¦¬í¬í„°ì˜ ëª©ì†Œë¦¬ë§Œ í¬í•¨ë©ë‹ˆë‹¤
2. ë°°ê²½ ìŒì•… ì ˆëŒ€ ê¸ˆì§€ (NO background music)
3. íš¨ê³¼ìŒ ì ˆëŒ€ ê¸ˆì§€ (NO sound effects)
4. í™˜ê²½ìŒ/ì£¼ë³€ ì†Œë¦¬ ì ˆëŒ€ ê¸ˆì§€ (NO ambient sounds)
5. "ìƒí™© :" ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì¥ë©´ì€ ì™„ì „íˆ ë¬´ìŒìœ¼ë¡œ ì²˜ë¦¬ (NO audio at all)

=== ğŸš« ìë§‰/í…ìŠ¤íŠ¸ ê¸ˆì§€ ===
â˜…â˜…â˜… ì˜ìƒì— ì–´ë–¤ í…ìŠ¤íŠ¸ë‚˜ ìë§‰ë„ ë„£ì§€ ë§ˆì„¸ìš” â˜…â˜…â˜…
- ìë§‰ ì—†ìŒ (NO subtitles)
- í™”ë©´ì— ê¸€ì ì—†ìŒ (NO on-screen text)
- ìº¡ì…˜ ì—†ìŒ (NO captions)
- ì˜¤ì§ ë§ˆì´í¬ì˜ "Gureumi TV" í…ìŠ¤íŠ¸ë§Œ í—ˆìš©

=== ì¥ë©´ ì„¤ëª… íƒœê·¸ ê·œì¹™ ===
- "ë¦¬í¬í„° : " = ë¦¬í¬í„°ì˜ ëŒ€ì‚¬ (ë¦¬í¬í„° ëª©ì†Œë¦¬ë¡œ ë§í•˜ê¸°)
- "êµ¬ë¦„ì´ : " = êµ¬ë¦„ì´ì˜ ëŒ€ì‚¬ (êµ¬ë¦„ì´ ëª©ì†Œë¦¬ë¡œ ë§í•˜ê¸°)
- "ìƒí™© : " = ëŒ€ì‚¬ ì—†ì´ í–‰ë™ë§Œ í‘œí˜„ (ì™„ì „ ë¬´ìŒ, ì›€ì§ì„ë§Œ)

=== ìºë¦­í„° ìŒì„± ì„¤ì • (ì „ì²´ ì˜ìƒì—ì„œ ì¼ê´€ë˜ê²Œ ìœ ì§€!) ===

1. êµ¬ë¦„ì´ (GOOREUM-I) ëª©ì†Œë¦¬:
   â˜… í•œêµ­ 4ì„¸ ì—¬ì ì•„ê¸° ëª©ì†Œë¦¬ - ì˜ìƒ ì²˜ìŒë¶€í„° ëê¹Œì§€ ë™ì¼í•˜ê²Œ! â˜…
   - ìŒë†’ì´: ë§¤ìš° ë†’ê³  ê·€ì—¬ìš´ ì•„ê¸° ëª©ì†Œë¦¬ (ì„±ì¸ì´ í‰ë‚´ë‚´ëŠ” ê²ƒì´ ì•„ë‹Œ ì§„ì§œ ì•„ê¸°ì²˜ëŸ¼)
   - ë°œìŒ: í˜€ ì§§ì€ ë°œìŒ (ì¢‹ì•„â†’ì¡°ì•„, ë­í•´â†’ë¨¸í•´, ì§„ì§œâ†’ì°ì§œ)
   - ë§íˆ¬: "~í•´ìš”", "~ì˜ˆìš”?", "íˆíˆ", "ì—í—¤í—¤~", "ì¡°ì•„ì¡°ì•„!"
   - íŠ¹ì§•: ì²œì§„ë‚œë§Œí•˜ê³ , ë°ê³ , ì• êµ ë„˜ì¹˜ëŠ” í†¤
   
2. ë¦¬í¬í„° ëª©ì†Œë¦¬:
   â˜… 20ì‚´ í•œêµ­ ì—¬ì„± ë¦¬í¬í„° - "Gureumi TV" ë§ˆì´í¬ í•„ìˆ˜! â˜…
   - ë‚˜ì´: 20ì‚´ (ì Šê³  ë°œë„í•œ í•œêµ­ ì—¬ì„±)
   - ì™¸ëª¨: ì „ë¬¸ì ì´ë©´ì„œë„ ì Šê³  ë°ì€ ëŠë‚Œì˜ ì—¬ì„± ë¦¬í¬í„°
   - ëª©ì†Œë¦¬: ì¹œê·¼í•˜ë©´ì„œë„ ë°©ì†¡ì¸ë‹¤ìš´ ë§íˆ¬
   - ë§ˆì´í¬ì— "Gureumi TV" ë¸Œëœë”©ì´ ë°˜ë“œì‹œ ë³´ì—¬ì•¼ í•¨

=== ì¤‘ìš” ê·œì¹™ ===
- ë§ˆì´í¬ì— "Gureumi TV" í‘œì‹œê°€ ë³´ì—¬ì•¼ í•¨
- êµ¬ë¦„ì´ëŠ” ì•‰ê±°ë‚˜ ëˆ„ìš´ ìì„¸ ìœ ì§€
- ëŒ€ì‚¬í•  ë•Œ ë§ˆì´í¬ë¥¼ ë§í•˜ëŠ” ìºë¦­í„° ì… ê·¼ì²˜ì— ìœ„ì¹˜
- â˜… êµ¬ë¦„ì´ ì™¸ëª¨: ì‘ê³  ê·€ì—¬ìš´ í•˜ì–€ í¬ë©”ë¼ë‹ˆì•ˆ (ì¥ë©´ ì„¤ëª…ì—ì„œ ì§€ì •í•œ ì˜· ì°©ìš©) â˜…

=== ì¥ë©´ êµ¬ì„± (${durationSeconds}ì´ˆ ì˜ìƒ) ===

${sceneDescriptions}

=== ìµœì¢… í™•ì¸ì‚¬í•­ ===
âœ“ ì´ ì˜ìƒ ê¸¸ì´: ${durationSeconds}ì´ˆ
âœ“ êµ¬ë¦„ì´ ëª©ì†Œë¦¬: ì˜ìƒ ì „ì²´ì—ì„œ ë™ì¼í•œ 4ì„¸ ì•„ê¸° ëª©ì†Œë¦¬ ìœ ì§€
âœ“ ë¦¬í¬í„° ëª©ì†Œë¦¬: ì˜ìƒ ì „ì²´ì—ì„œ ë™ì¼í•œ ëª©ì†Œë¦¬ ìœ ì§€
âœ“ ë§ˆì§€ë§‰ ì¥ë©´ì—ì„œë§Œ í•¨ê»˜ ë¯¸ì†Œ
âœ“ ë°°ê²½ìŒì•…/íš¨ê³¼ìŒ/ì£¼ë³€ì†Œë¦¬ ì—†ìŒ - ì˜¤ì§ ëŒ€ì‚¬ë§Œ
âœ“ ìë§‰/í…ìŠ¤íŠ¸ ì—†ìŒ
âœ“ "ìƒí™© :" ì¥ë©´ì€ ì™„ì „ ë¬´ìŒ`;

    return unifiedPrompt;
}

// Generate a single video with all scenes combined (for voice consistency)
export async function generateUnifiedVideo(
    apiKey: string,
    imageData: ImageData,
    prompts: string[],
    durationSeconds: number,
    onProgress: (message: string) => void,
    shouldStop?: () => boolean
): Promise<GeneratedVideo> {
    const ai = new GoogleGenAI({ apiKey });

    // Build unified prompt with all scenes
    const unifiedPrompt = buildUnifiedVideoPrompt(prompts, durationSeconds);

    onProgress('ì˜ìƒ ìƒì„± ìš”ì²­ ì¤‘...');
    console.log('Unified prompt:', unifiedPrompt);

    try {
        // Generate single video with the unified prompt
        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        let operation: any = await ai.models.generateVideos({
            model: 'veo-3.1-generate-preview',
            prompt: unifiedPrompt,
            image: {
                imageBytes: imageData.base64,
                mimeType: imageData.mimeType,
            },
            config: {
                aspectRatio: imageData.aspectRatio,
                numberOfVideos: 1,
                durationSeconds: durationSeconds,
            },
        });

        // Poll until done
        let pollCount = 0;
        while (!operation.done) {
            // Check if we should stop during polling
            if (shouldStop?.()) {
                throw new Error('ìƒì„±ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.');
            }
            pollCount++;
            onProgress(`ì˜ìƒ ì²˜ë¦¬ ì¤‘... (${pollCount * 10}ì´ˆ ê²½ê³¼)`);
            await new Promise((resolve) => setTimeout(resolve, 10000));
            operation = await ai.operations.getVideosOperation({ operation });
        }

        // Get the generated video
        const generatedVideoResponse = operation.response.generatedVideos[0];
        console.log('Generated video response:', generatedVideoResponse);

        onProgress('ì˜ìƒ ë‹¤ìš´ë¡œë“œ ì¤‘...');

        // Download the video
        const videoBlob = await downloadVideoFromResponse(
            generatedVideoResponse.video,
            apiKey,
            onProgress
        );

        const videoUrl = URL.createObjectURL(videoBlob);

        const video: GeneratedVideo = {
            id: 'unified-video',
            videoUrl,
            videoBlob,
            prompt: unifiedPrompt,
            step: 1,
        };

        onProgress('ì˜ìƒ ìƒì„± ì™„ë£Œ!');
        return video;

    } catch (error) {
        console.error('Error generating unified video:', error);
        throw new Error(`ì˜ìƒ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: ${error instanceof Error ? error.message : String(error)}`);
    }
}


export async function generateVideosSequentially(
    apiKey: string,
    imageData: ImageData,
    prompts: string[],
    onProgress: (step: number, message: string) => void,
    onVideoGenerated: (video: GeneratedVideo) => void,
    shouldStop?: () => boolean
): Promise<GeneratedVideo[]> {
    const ai = new GoogleGenAI({ apiKey });
    const generatedVideos: GeneratedVideo[] = [];

    const totalVideos = prompts.length;

    for (let i = 0; i < totalVideos; i++) {
        // Check if we should stop before starting next video
        if (shouldStop?.()) {
            onProgress(i + 1, 'ìƒì„±ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.');
            break;
        }

        const userPrompt = prompts[i];
        const isLastScene = i === totalVideos - 1;

        // Build enhanced prompt with critical rules
        const enhancedPrompt = buildEnhancedVideoPrompt(userPrompt, i + 1, totalVideos, isLastScene);

        onProgress(i + 1, `ë™ì˜ìƒ ${i + 1}/${totalVideos} ìƒì„± ìš”ì²­ ì¤‘...`);

        try {
            // eslint-disable-next-line @typescript-eslint/no-explicit-any
            let operation: any;

            // All videos start from the same original image (independent generation)
            // This allows users to edit videos separately in external software
            operation = await ai.models.generateVideos({
                model: 'veo-3.1-generate-preview',
                prompt: enhancedPrompt,
                image: {
                    imageBytes: imageData.base64,
                    mimeType: imageData.mimeType,
                },
                config: {
                    aspectRatio: imageData.aspectRatio,
                    numberOfVideos: 1,
                },
            });

            // Poll until done
            let pollCount = 0;
            while (!operation.done) {
                // Check if we should stop during polling
                if (shouldStop?.()) {
                    onProgress(i + 1, 'ìƒì„±ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.');
                    return generatedVideos;
                }
                pollCount++;
                onProgress(i + 1, `ë™ì˜ìƒ ${i + 1}/${totalVideos} ì²˜ë¦¬ ì¤‘... (${pollCount * 10}ì´ˆ ê²½ê³¼)`);
                await new Promise((resolve) => setTimeout(resolve, 10000));
                operation = await ai.operations.getVideosOperation({ operation });
            }

            // Get the generated video
            const generatedVideoResponse = operation.response.generatedVideos[0];
            console.log('Generated video response:', generatedVideoResponse);
            console.log('Video object:', generatedVideoResponse?.video);



            onProgress(i + 1, `ë™ì˜ìƒ ${i + 1}/${totalVideos} ë‹¤ìš´ë¡œë“œ ì¤‘...`);

            // Download the video using our helper function
            const videoBlob = await downloadVideoFromResponse(
                generatedVideoResponse.video,
                apiKey,
                (msg: string) => onProgress(i + 1, msg)
            );

            const videoUrl = URL.createObjectURL(videoBlob);

            const video: GeneratedVideo = {
                id: `video-${i + 1}`,
                videoUrl,
                videoBlob,
                prompt: userPrompt,
                step: i + 1,
            };

            generatedVideos.push(video);
            onVideoGenerated(video);
            onProgress(i + 1, `ë™ì˜ìƒ ${i + 1}/${totalVideos} ì™„ë£Œ!`);

        } catch (error) {
            console.error(`Error generating video ${i + 1}:`, error);
            throw new Error(`ë™ì˜ìƒ ${i + 1} ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: ${error instanceof Error ? error.message : String(error)}`);
        }
    }

    return generatedVideos;
}


// Helper function to detect aspect ratio from image dimensions
export function detectAspectRatio(width: number, height: number): '16:9' | '9:16' {
    const ratio = width / height;

    // If wider than tall, use 16:9, otherwise use 9:16
    if (ratio >= 1) {
        return '16:9';
    } else {
        return '9:16';
    }
}

// Helper function to convert file to base64
export function fileToBase64(file: File): Promise<{ base64: string; mimeType: string }> {
    return new Promise((resolve, reject) => {
        const reader = new FileReader();
        reader.onload = () => {
            const result = reader.result as string;
            // Remove the data URL prefix to get just the base64 data
            const base64 = result.split(',')[1];
            resolve({ base64, mimeType: file.type });
        };
        reader.onerror = reject;
        reader.readAsDataURL(file);
    });
}

// Helper function to get image dimensions
export function getImageDimensions(file: File): Promise<{ width: number; height: number }> {
    return new Promise((resolve, reject) => {
        const img = new Image();
        img.onload = () => {
            resolve({ width: img.width, height: img.height });
            URL.revokeObjectURL(img.src);
        };
        img.onerror = reject;
        img.src = URL.createObjectURL(file);
    });
}

// Generate dialogue recommendations for the final video
export async function generateDialogueRecommendations(
    apiKey: string,
    prompts: string[],
    onProgress: (message: string) => void
): Promise<{ evaluationCriteria: string[]; recommendations: { dialogue: string; timingSeconds: number; reasoning: string }[] }> {
    const { GoogleGenerativeAI } = await import('@google/generative-ai');
    const genAI = new GoogleGenerativeAI(apiKey);
    const model = genAI.getGenerativeModel({ model: 'gemini-2.0-flash' });

    onProgress('ëŒ€ì‚¬ ë¶„ì„ì„ ìœ„í•œ ê¸°ì¤€ì„ ìˆ˜ë¦½ ì¤‘...');

    // Build dynamic prompts list
    const promptsList = prompts.map((p, i) => `${i + 1}. ${p}`).join('\n');
    const lastVideoNum = prompts.length;

    const prompt = `ë‹¹ì‹ ì€ ë™ì˜ìƒ ì½˜í…ì¸  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ëŠ” ${prompts.length}ê°œì˜ ì—°ì† ë™ì˜ìƒì— ëŒ€í•œ í”„ë¡¬í”„íŠ¸ì…ë‹ˆë‹¤.
ë§ˆì§€ë§‰ ë™ì˜ìƒ(ë™ì˜ìƒ ${lastVideoNum})ì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ë“±ì¥ì¸ë¬¼ì´ ë§í•  ìˆ˜ ìˆëŠ” **ìµì‚´ìŠ¤ëŸ½ê³  ì¬ë¯¸ìˆëŠ” ëŒ€ì‚¬**ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.

## ë™ì˜ìƒ í”„ë¡¬í”„íŠ¸ë“¤:
${promptsList}

## ì¤‘ìš” ìºë¦­í„° ì •ë³´:
- **êµ¬ë¦„ì´**: ë™ì˜ìƒì— ë“±ì¥í•˜ëŠ” ê·€ì—¬ìš´ í•˜ì–€ìƒ‰ í¬ë©”ë¼ë‹ˆì•ˆ ê°•ì•„ì§€ì…ë‹ˆë‹¤.
- **êµ¬ë¦„ì´ì˜ ëª©ì†Œë¦¬ íŠ¹ì„±**: êµ¬ë¦„ì´ëŠ” **í•œêµ­ì˜ 4ì„¸ ì—¬ì ì•„ê¸°ì²˜ëŸ¼ ì—„ì²­ ê·€ì—½ê³  ì²œì§„ë‚œë§Œí•˜ê²Œ** ë§í•©ë‹ˆë‹¤.
  
  === ëª©ì†Œë¦¬ & ë§íˆ¬ í•„ìˆ˜ ê·œì¹™ ===
  1. **ë°œìŒ**: í˜€ ì§§ì€ ì†Œë¦¬ë¡œ ê·€ì—½ê²Œ (ì˜ˆ: "ì‚¬ë‘í•´" â†’ "ì‹¸ë‘í•´", "ë­í•´?" â†’ "ë¨¸í•´?", "ì§„ì§œ?" â†’ "ì°ì§œ?")
  2. **ë¬¸ì¥ ëë§ºìŒ**: 
     - "~í•´ìš”", "~ì˜ˆìš”", "~ê±°ì˜ˆìš”?", "~í• ë˜ìš”!", "~ì¢‹ì•„ìš”!"
     - "~í•˜ëŠ” ê±°ì•¼?", "~ì¸ ê±°ì•¼?", "~í•´ì¤˜ìš”~"
  3. **ê°íƒ„ì‚¬/ì¶”ì„ìƒˆ**: "ì™€ì•„~!", "ìš°ì™€~!", "í—¤í—¤", "íˆíˆ", "ìœ¼ì•™~", "ì—í—¤í—¤~", "ìŒëƒ ëƒ ", "ì•¼í˜¸~!"
  4. **ê·€ì—¬ìš´ í‘œí˜„ë“¤**:
     - "ë‚˜ ì´ê±° ì¡°ì•„ì¡°ì•„!" (ì¢‹ì•„ì¢‹ì•„)
     - "ë„ˆë¬´ë„ˆë¬´ í–‰ë³µí•´ìš”~"
     - "ì´ê²Œ ë­ì•¼? ì´ê²Œ ë¨¸ëƒê³ ìš§!"
     - "ì—„ë§ˆì•„~ ì•„ë¹ ì•„~"
     - "ê°„ì‹ ì£¼ì„¸ìš”ìš”~ ì‘?"
     - "êµ¬ë¦„ì´ ì°©í•˜ì£ ? ì‘ì‘?"
     - "ë†€ì•„ì¤˜ìš”~ ì‹¬ì‹¬í•´ìš”~"
  5. **íŠ¹ì§•ì  ë§ë²„ë¦‡**:
     - ë¬¸ì¥ ëì— "~ìš”" ë˜ëŠ” "~ìš©" ë¶™ì´ê¸°
     - ê°™ì€ ë§ ë°˜ë³µ (ì˜ˆ: "ë§›ìˆë‹¤ ë§›ìˆì–´!", "ì¡°ì•„ì¡°ì•„!")
     - ì§ˆë¬¸í•  ë•Œ ê³ ê°œ ê°¸ì›ƒí•˜ëŠ” ëŠë‚Œ "~ì¸ ê±°ì•¼?"
  6. **í†¤**: ë§¤ìš° ë°ê³ , ì‹ ë‚˜ê³ , ì• êµ ë„˜ì¹˜ê³ , ë•Œë¡œëŠ” íˆ¬ì • ë¶€ë¦¬ë“¯ì´

  === ì ˆëŒ€ í•˜ë©´ ì•ˆ ë˜ëŠ” ê²ƒ ===
  - ì–´ë¥¸ìŠ¤ëŸ¬ìš´ ë§íˆ¬ ì‚¬ìš© ê¸ˆì§€
  - ë„ˆë¬´ ê¸´ ë¬¸ì¥ ê¸ˆì§€ (4ì„¸ ì•„ì´ëŠ” ì§§ê²Œ ë§í•¨)
  - ê²½ì–´ì™€ ë°˜ë§ ì„ì–´ ì“°ëŠ” ê±´ OK (ì•„ì´ë“¤ íŠ¹ì§•)

## ìš”ì²­ì‚¬í•­:
1. **ë¨¼ì €**, ìµì‚´ìŠ¤ëŸ½ê³  ì¬ë¯¸ ìš”ì†Œê°€ ì¶©ë¶„í•œ ëŒ€ì‚¬ì¸ì§€ í‰ê°€í•˜ê¸° ìœ„í•œ **í‰ê°€ ê¸°ì¤€ 3~5ê°œ**ë¥¼ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”.
2. **ê·¸ ë‹¤ìŒ**, ìœ„ ê¸°ì¤€ì„ ëª¨ë‘ ë§Œì¡±í•˜ëŠ” ëŒ€ì‚¬ **2~3ê°œ**ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.
3. êµ¬ë¦„ì´(í¬ë©”ë¼ë‹ˆì•ˆ)ê°€ ë§í•˜ëŠ” ê²½ìš°, ë°˜ë“œì‹œ ìœ„ì˜ **4ì„¸ ì—¬ì ì•„ê¸° ëª©ì†Œë¦¬ íŠ¹ì„±**ì„ 100% ë°˜ì˜í•´ì£¼ì„¸ìš”!
4. ê° ëŒ€ì‚¬ì— ëŒ€í•´:
   - ëŒ€ì‚¬ ë‚´ìš© (ëˆ„ê°€ ë§í•˜ëŠ”ì§€ ëª…ì‹œ)
   - í•´ë‹¹ ëŒ€ì‚¬ê°€ ë“¤ì–´ê°€ë©´ ì¢‹ì„ ì‹œì  (ë™ì˜ìƒ ${lastVideoNum}ì´ ì•½ 8ì´ˆë¼ê³  ê°€ì •, 0~8ì´ˆ ì‚¬ì´)
   - ì™œ ì´ ëŒ€ì‚¬ê°€ ì¬ë¯¸ìˆëŠ”ì§€ ê°„ë‹¨í•œ ì´ìœ 

## ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ ì´ JSON í˜•ì‹ì„ ë”°ë¼ì£¼ì„¸ìš”):
\`\`\`json
{
  "evaluationCriteria": [
    "ê¸°ì¤€1: ì„¤ëª…",
    "ê¸°ì¤€2: ì„¤ëª…",
    "ê¸°ì¤€3: ì„¤ëª…"
  ],
  "recommendations": [
    {
      "dialogue": "ì¶”ì²œ ëŒ€ì‚¬ 1 (í™”ì: êµ¬ë¦„ì´/ê¸°ì/ë“±)",
      "timingSeconds": 2,
      "reasoning": "ì´ ëŒ€ì‚¬ê°€ ì¬ë¯¸ìˆëŠ” ì´ìœ "
    },
    {
      "dialogue": "ì¶”ì²œ ëŒ€ì‚¬ 2",
      "timingSeconds": 5,
      "reasoning": "ì´ ëŒ€ì‚¬ê°€ ì¬ë¯¸ìˆëŠ” ì´ìœ "
    }
  ]
}
\`\`\`

ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì•ˆì— JSONì„ ë„£ì–´ì£¼ì„¸ìš”.`;

    onProgress('Gemini APIë¡œ ëŒ€ì‚¬ë¥¼ ë¶„ì„ ì¤‘...');

    const result = await model.generateContent(prompt);
    const response = await result.response;
    const text = response.text();

    onProgress('ì‘ë‹µì„ íŒŒì‹± ì¤‘...');

    // Extract JSON from markdown code block if present
    const jsonMatch = text.match(/```(?:json)?\s*([\s\S]*?)```/);
    const jsonStr = jsonMatch ? jsonMatch[1].trim() : text.trim();

    try {
        const parsed = JSON.parse(jsonStr);
        return {
            evaluationCriteria: parsed.evaluationCriteria || [],
            recommendations: parsed.recommendations || []
        };
    } catch {
        console.error('Failed to parse response:', text);
        throw new Error('ëŒ€ì‚¬ ì¶”ì²œ ì‘ë‹µì„ íŒŒì‹±í•˜ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.');
    }
}

// Generate interview ideas for reporter and Gooreum-i (Pomeranian dog)
export async function generateInterviewIdeas(
    apiKey: string,
    onProgress: (message: string) => void
): Promise<{ location: string; time: string; situation: string; reporterDialogue: string; gooreumiDialogue: string }[]> {
    const { GoogleGenerativeAI } = await import('@google/generative-ai');
    const genAI = new GoogleGenerativeAI(apiKey);
    const model = genAI.getGenerativeModel({ model: 'gemini-2.0-flash' });

    onProgress('ì•„ì´ë””ì–´ ìƒì„± ì¤‘...');

    const prompt = `ë‹¹ì‹ ì€ ìœ íŠœë¸Œ ì±„ë„ "êµ¬ë¦„í„°"ì˜ ì½˜í…ì¸  ê¸°íšìì…ë‹ˆë‹¤. 
êµ¬ë¦„í„° ì±„ë„ì€ ë¦¬í¬í„°ê°€ ê·€ì—¬ìš´ í°ìƒ‰ í¬ë©”ë¼ë‹ˆì•ˆ ê°•ì•„ì§€ "êµ¬ë¦„ì´"ë¥¼ ì¸í„°ë·°í•˜ëŠ” í˜•ì‹ì˜ ìˆí¼ ì˜ìƒì„ ë§Œë“­ë‹ˆë‹¤.

## ì˜ìƒ ì»¨ì…‰:
- ë¦¬í¬í„°ê°€ ë§ˆì´í¬("Gureumi TV")ë¥¼ ë“¤ê³  êµ¬ë¦„ì´ì—ê²Œ ë‹¤ì–‘í•œ ìƒí™©ì—ì„œ ì¸í„°ë·°í•©ë‹ˆë‹¤
- êµ¬ë¦„ì´ëŠ” 4ì„¸ ì•„ê¸°ì²˜ëŸ¼ ê·€ì—½ê³  ì—‰ëš±í•˜ê²Œ ëŒ€ë‹µí•©ë‹ˆë‹¤
- êµ¬ë¦„ì´ì˜ ë§ˆì§€ë§‰ ëŒ€ë‹µì€ í•­ìƒ ìœ„íŠ¸ìˆê³  ì¬ë¯¸ìˆì–´ì•¼ í•©ë‹ˆë‹¤

## êµ¬ë¦„ì´ ë§íˆ¬ íŠ¹ì„±:
- í˜€ ì§§ì€ ì†Œë¦¬ (ì˜ˆ: "ì¢‹ì•„" â†’ "ì¡°ì•„", "ë­í•´?" â†’ "ë¨¸í•´?")
- ê·€ì—¬ìš´ ë¬¸ì¥ ëë§ºìŒ: "~í•´ìš”", "~ì˜ˆìš”", "~í• ë˜ìš”!", "~ê±°ì˜ˆìš”?"
- ê°íƒ„ì‚¬: "ì™€ì•„~!", "í—¤í—¤", "íˆíˆ", "ì—í—¤í—¤~", "ì•¼í˜¸~!"
- ê°™ì€ ë§ ë°˜ë³µ: "ë§›ìˆë‹¤ë§›ìˆì–´!", "ì¡°ì•„ì¡°ì•„!"
- ì§§ê³  ê·€ì—¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ë§í•¨

## ìš”ì²­ì‚¬í•­:
ë‹¤ì–‘í•˜ê³  ì°½ì˜ì ì¸ ì¸í„°ë·° ìƒí™© 5ê°œë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”. 
ê° ìƒí™©ì€ ìƒ‰ë‹¤ë¥´ê³  ì¬ë¯¸ìˆì–´ì•¼ í•˜ë©°, ë§ˆì§€ë§‰ êµ¬ë¦„ì´ì˜ ëŒ€ë‹µì€ ì˜ˆìƒì„ ë²—ì–´ë‚˜ëŠ” ìœ„íŠ¸ìˆëŠ” ë‹µë³€ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

## ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ JSONìœ¼ë¡œ):
\`\`\`json
[
  {
    "location": "ì¥ì†Œ (ì˜ˆ: ë†€ì´ê³µì›, ë™ë¬¼ë³‘ì›, ì¹´í˜ ë“±)",
    "time": "ì‹œê°„ëŒ€ (ì˜ˆ: í™”ì°½í•œ ì˜¤í›„, ëˆˆ ì˜¤ëŠ” ì €ë… ë“±)",
    "situation": "ìƒí™© ì„¤ëª… (20ì ì´ë‚´)",
    "reporterDialogue": "ë¦¬í¬í„°ì˜ ì§ˆë¬¸ ë˜ëŠ” ëŒ€í™” (ì˜ˆ: 'êµ¬ë¦„ì´, ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë•Œìš”?')",
    "gooreumiDialogue": "êµ¬ë¦„ì´ì˜ ìœ„íŠ¸ìˆê³  ì¬ë¯¸ìˆëŠ” ëŒ€ë‹µ (4ì„¸ ì•„ê¸° ë§íˆ¬ë¡œ)"
  }
]
\`\`\`

5ê°œì˜ ë‹¤ì–‘í•œ ìƒí™©ì„ JSON ë°°ì—´ë¡œ ì œì•ˆí•´ì£¼ì„¸ìš”.`;

    const result = await model.generateContent(prompt);
    const response = await result.response;
    const text = response.text();

    onProgress('ì‘ë‹µ íŒŒì‹± ì¤‘...');

    // Extract JSON from markdown code block if present
    const jsonMatch = text.match(/```(?:json)?\s*([\s\S]*?)```/);
    const jsonStr = jsonMatch ? jsonMatch[1].trim() : text.trim();

    try {
        const parsed = JSON.parse(jsonStr);
        return Array.isArray(parsed) ? parsed : [];
    } catch {
        console.error('Failed to parse interview ideas response:', text);
        throw new Error('ì•„ì´ë””ì–´ ìƒì„± ì‘ë‹µì„ íŒŒì‹±í•˜ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.');
    }
}

